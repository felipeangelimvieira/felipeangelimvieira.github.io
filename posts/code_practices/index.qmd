---
title: "Can your code pass the test set?"
format:
  html:
    toc: true
    toc-depth: 3
    toc-expand: 3
author: "Felipe Angelim"
date: "2025-06-10"
draft: false
categories: [software design, machine learning]
jupyter: python3
bibliography: bibliography.bib
---


Code is not meant to be written once and never touched again. It is meant to be read, understood, and modified over time. This is especially true in dynamic environments, where requirements change, new features are added, and bugs are fixed. In such environments, code design quality is not just a nice-to-have; it is essential for the long-term success of the project.

Here, I wish to share some thoughts on code design qualities, focusing on what we want, not how to get there. This is not a guide on how to write good code but rather a reflection on what good code design means in the context of software engineering and data science. This can also be useful for managers, product owners, and stakeholders who do not code but want to understand how quality impacts the product they are building.

The key takeaway I wish you to have is that **good code design is about resilience**. It is about not overfitting to our myopic view of the present, but rather about crafting a solution that can adapt and generalize to future requirements, just like a good machine learning model. It is about how effortless it is to adapt to changes.

## The map is not the territory

![](imgs/map_territory.png){width=30% fig-align="center" style="border-radius: 40px;"}

There is this nice quote from Alfred Korzybski that says "the map is not the territory". This means that any representation of reality is not reality itself, but rather a simplification of it. Also, it is shaped by our biases and limitations.

Bringing it to the context of this post, we can say that **product requirements are not the product we are building**. They are a simplification of what we want to build. Naturally, details can be missing and assumptions can be wrong, because sometimes reality is more complex than what we can express in words. Sometimes, we just don't know, and that is okay. We just have to be cautious not to mistake the empty with the void.

This idea is key to understanding what comes next. Our code is designed to solve problems, but our definitions of problems are not perfect. They are made by humans, and hence this human aspect and the imprecise understanding of what should be done is deeply connected to the quality of the code we write, even more so than what our linter and unit tests end up being.

## Easy is not simple

Before using the word "simple" to describe code, we should properly define what we mean by that. As gloriously explained by [Rich Hickey](https://www.youtube.com/watch?v=rI8tNMsozo0) in his talk "Simplicity Matters," simple is not the same as easy. Simple is about not packing together unrelated concerns; it is about giving clear responsibility to each component, and making it easy to modify and extend. Easy, on the other hand, is subjective; it is what is closer to your knowledge and experience, it is what you are used to doing, and not necessarily good. We should focus here on the code, not on our perspective of knowledge and experience.

This definition aligns with the one in the book "A Philosophy of Software Design" (@ousterhout2018philosophy), which is a huge inspiration and reference for this post.

> Complexity is anything related to the structure of a software system that makes it hard to understand and modify the system

Complexity is not related to the number of lines or functions; it is about the bigger picture. In his book, John Ousterhout also enumerates some symptoms of complexity:

* Change amplification: when a change in one part of the system requires changes in many other parts.
* Cognitive load: when the code is hard to understand, requiring a lot of mental effort to comprehend.
* Unknown unknowns: when it is hard to diagnose where a change is needed to complete a task.

Basically, simplicity implies less effort in the long term.

Think about the code as a graph. The nodes are components (e.g., functions) we create, and the edges represent their dependencies. Simple code would be a graph with few edges, where edges represent dependencies of components on others. A dependency can be, for example, an assumption of the structure of the output of one component, the existence of certain methods, etc. The number of edges would be proportional to the complexity of the system and our cognitive load.


```{python}
# | echo: false
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np

# --- Re‑create the clustered graph ---------------------------------
sizes = [4, 4, 4]
p_intra, p_inter = 0.85, 0.04
G = nx.random_partition_graph(sizes, p_intra, p_inter, seed=33)

# Add the requested cross‑cluster edge (5‑8)
G.add_edge(5, 8)

# Identify node indices per cluster
partitions = list(G.graph["partition"])
clusters = {cid: list(block) for cid, block in enumerate(partitions)}

# Position clusters
cluster_offsets = np.array([[-2, 0], [2, 0], [0, 3]])
pos = {}
for cid, nodes in clusters.items():
    sub_pos = nx.spring_layout(G.subgraph(nodes), seed=cid, scale=1.0)
    offset = cluster_offsets[cid]
    for n in nodes:
        pos[n] = sub_pos[n] + offset

# Draw updated graph
plt.figure(figsize=(5, 5))
nx.draw(
    G,
    pos,
    with_labels=True,
    node_size=700,
    node_color="skyblue",
    edge_color="gray",
    linewidths=0.8,
)
plt.title("Loosely Coupled Code", pad=15)
plt.show()
```

If we would perturb the graph above, such as changing the output of node 11, it would only affect the nodes that are directly connected to it, and not the whole graph. This is what we want to achieve with our code: a graph with few edges, where we can easily replace parts without affecting the whole system.


```{python}
# | echo: false

import matplotlib.pyplot as plt
import networkx as nx
import numpy as np

# === Rebuild the clustered graph (same seed/layout) ===============
sizes = [4, 4, 4]
p_intra, p_inter = 0.85, 0.04
G = nx.random_partition_graph(sizes, p_intra, p_inter, seed=33)
G.add_edge(5, 8)  # the cross-cluster link

# Find partitions (each cluster's nodes)
partitions = list(G.graph["partition"])

# Layout clusters with spacing
cluster_offsets = np.array([[-2, 0], [2, 0], [0, 3]])
pos = {}
for cid, nodes in enumerate(partitions):
    sub_pos = nx.spring_layout(G.subgraph(nodes), seed=cid, scale=1.0)
    offset = cluster_offsets[cid]
    for n in nodes:
        pos[n] = sub_pos[n] + offset

# Identify node 11 and its direct neighbours
target_node = 11
neighbors = list(G.neighbors(target_node))

# Drawing ----------------------------------------------------------
plt.figure(figsize=(5, 5))

# Draw all edges first
nx.draw_networkx_edges(G, pos, edge_color="gray", width=1.0)

# Draw non-affected nodes
non_affected = [n for n in G.nodes if n not in neighbors + [target_node]]
nx.draw_networkx_nodes(
    G, pos, nodelist=non_affected, node_size=700, node_color="skyblue", linewidths=0.8
)

# Draw neighbours in light orange
nx.draw_networkx_nodes(
    G, pos, nodelist=neighbors, node_size=700, node_color="#FFD59B", linewidths=0.8
)

# Draw target node in orange/red
nx.draw_networkx_nodes(
    G, pos, nodelist=[target_node], node_size=900, node_color="#FF8C42", linewidths=1.0
)

# Labels
nx.draw_networkx_labels(G, pos, font_size=10, font_color="black")

# Add a wrench / gear emoji right above node 11
x, y = pos[target_node]
plt.text(x, y + 0.35, "\u2699", fontsize=24, ha="center")

plt.title("Perturbing a Component and Its Ripple Effect", pad=15)
plt.axis("off")
plt.show()

```

## The objective function

Here, I'll try to define a conceptual "objective function"—not mathematical, but rather a guiding principle that should shape how we design and structure our code.

### Overfitting to training data

At first glance, you might think that code is good if it simply does what it's supposed to do. However, this perspective often encourages quick-and-dirty solutions and spaghetti code that might solve immediate problems but overlook the bigger picture.

![](imgs/illustration1.png){width=70% fig-align="center"}

Imagine an abstract space of requirements, like the one above. We can only see a tiny fraction of our long-term goals clearly. Focusing too narrowly on the present moment—what we already know—makes us overfit to our current understanding, limiting our ability to adapt to new challenges. **The more complex and specifically tuned code is, the more likely it is to overfit** and struggle when things inevitably change.

![](imgs/illustration2.png){width=70% fig-align="center"}

While "easy" solutions may feel quicker and simpler initially, generalizable, well-designed solutions usually save more effort in the long run. They avoid the exponential increase in complexity and maintenance costs that come from constantly adapting overfitted solutions.

```{python}
# | echo: false
# | fig-align: "center"
import matplotlib.pyplot as plt
import pandas as pd

# Timeline of requirement changes
dates = pd.to_datetime(
    [
        "2025-01-01",
        "2025-01-15",
        "2025-02-10",
        "2025-03-15",
        "2025-04-20",
        "2025-05-27",
    ]
)

# Cumulative effort values
overfitted_cum = [0, 3, 10, 15, 25, 50]
robust_cum = [0, 12, 13, 13.5, 14, 14.5]

# Regret = extra effort caused by overfitting
regret = [o - r for o, r in zip(overfitted_cum, robust_cum)]

# Plot
fig, ax = plt.subplots(figsize=(8, 3))
ax.plot(dates, overfitted_cum, marker="o", label="Overfitted code")
ax.plot(dates, robust_cum, marker="o", label="Well-designed code")

for d in dates[1:]:
    ax.axvline(d, linestyle="--", linewidth=0.8, alpha=0.3, color="black")

ax.set_ylabel("Effort (relative units)")
ax.set_title("Effort required over time as requirements evolve")
ax.legend()
ax.grid(False)
plt.show()
```

### The regret

I've often wondered what exactly we're optimizing for when designing and writing code. Just like machine learning models use metrics to evaluate their performance, we need something to measure code quality. After some thought, I've come to see *regret* as a useful measure.

In simple terms, regret is the extra effort spent compared to an ideal solution. This concept is borrowed from machine learning, particularly from the "multi-armed bandit" problem, where decisions must be made based on limited information. The goal there is to minimize regret—the gap between your choices and the best possible outcomes—over time. This mirrors how we write code, making choices with incomplete information and constantly adapting as we learn more.

Experienced developers naturally have lower regret because they anticipate changes more effectively, ask better questions early on, and build resilient systems from the start.

```{python}
# | echo: false
# | fig-align: "center"

# Calculate the cumulative regret
cumulative_regret = [sum(regret[: i + 1]) for i in range(len(regret))]

fig, ax = plt.subplots(figsize=(8, 4))
ax.fill_between(dates, regret, alpha=0.25, label="Regret", color="blue")
ax.plot(dates, regret, marker="o", linewidth=1)
ax.set_ylabel("Regret")
ax.set_title("Regret (extra effort due to overfitting)")
ax.legend()
ax.grid(False)
fig.autofmt_xdate()
fig.tight_layout()
plt.show()
```

### Good code starts before writing code

Since we rarely have full information upfront, we shouldn't aim for perfect code immediately. Instead, the first thing we should do is **ask questions**:

* What requirements are likely to change?
* What requirements are unlikely to change?
* What assumptions are we making?
* What is the expected lifespan of the product?

With the answers to these questions, we can start shaping our design. Once we have a draft proposal, we can evaluate how different possible future changes would affect the current structure.

![](imgs/darkroom.png){width=50% fig-align="center" style="border-radius: 40px;"}

Designing often involves identifying stable components and defining contracts between them. Asking the right questions early helps us group functionalities meaningfully and build systems that remain flexible over time.

### Avoid premature optimization

While it's important to anticipate change, guessing too far ahead about possible future requirements can be dangerous. This is known as *premature optimization*. It can create significant regret—first due to excessive effort upfront, and later because reality rarely matches our early guesses.

```{python}
# | echo: false
# | fig-align: "center"

import matplotlib.pyplot as plt
import pandas as pd

# Data
dates = pd.to_datetime(
    [
        "2025-01-01",
        "2025-01-15",
        "2025-02-10",
        "2025-03-15",
        "2025-04-20",
        "2025-05-27",
    ]
)

overfitted_cum = [0, 30, 35, 35, 35, 35]
robust_cum = [0, 12, 13, 13.5, 14, 14.5]

regret = [o - r for o, r in zip(overfitted_cum, robust_cum)]

# Plot
fig, axs = plt.subplots(nrows=2, sharex=True, figsize=(8, 6))

# Effort
ax_effort = axs[0]
ax_effort.plot(dates, overfitted_cum, marker="o", label="Premature optimization")
ax_effort.plot(dates, robust_cum, marker="o", label="Well-designed code")
ax_effort.set_ylabel("Effort (relative units)")
ax_effort.set_title("Effort vs. Regret as requirements evolve")
ax_effort.legend(loc="upper left")

# Regret
ax_regret = axs[1]
ax_regret.fill_between(dates, regret, alpha=0.25, label="Regret", color="blue")
ax_regret.set_ylabel("Regret (extra effort)")
ax_regret.legend(loc="upper left")

# Styling
for ax in axs:
    for d in dates[1:]:
        ax.axvline(d, linestyle="--", linewidth=0.8, alpha=0.3, color="black")
    ax.grid(False)

fig.autofmt_xdate()
fig.tight_layout()
plt.show()
```

I once found myself in this trap: trying to optimize a codebase that wasn't even in production yet. I went deep into SOLID principles, endlessly splitting functions and abstracting for flexibility I thought I might need later. But those future requirements never arrived. The code became so abstract that it was hard to maintain, and when real changes eventually came, I had to rewrite everything anyway.

## Conclusion

Writing good code isn’t just about making something that works today — it’s about building solutions that can adapt, evolve, and scale as requirements inevitably change. Much like training a machine learning model, if we overfit our code to the present, we may pay a heavy price in the future. Balancing simplicity, flexibility, and pragmatism is what leads to resilient systems with low long-term maintenance cost.

By focusing on minimizing *regret*—the extra effort caused by early design decisions that don’t age well—we can make smarter choices even when information is incomplete. And while experience helps us make better bets, asking the right questions early on dramatically increases our odds of success.

In the end, it’s not about writing perfect code — it’s about writing code that can gracefully survive its own future.

## Key Takeaways

- Code quality is about resilience, not just correctness 
- Overfitting code to today's requirements creates fragile systems.
- Minimize regret: optimize for adaptability, not hypothetical future needs.
- **Good design starts with asking the right questions, not jumping into abstractions.**
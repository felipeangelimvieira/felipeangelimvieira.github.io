---
title: "Bayesian Inference - Motivation and Basics"
description: "A primer on Bayesian Inference, its motivations, and how it works."
format:
  html:
    toc: true
    toc-depth: 4
    toc-expand: 4
author: "Felipe Angelim"
date: "2025-05-27"
categories: [machine learning, bayesian inference]
jupyter: python3
---

Everyone has heard about Bayesian Inference, but few people know what it is, how it works, and why it is useful. In this post, we will explore the basics of Bayesian Inference, focusing on the inference methodologies and their motivations. Here are some motivations of having it as a new tool in your toolbox:

* **Probabilistic programming is powerful**, and bayesian inference provides a solid foundation for it. With probabilistic programming, we can build models that can handle uncertainty, incorporate prior knowledge about the data generation process, and are interpretable. Bayesian Inference is the basis of many probabilistic programming libraries, such as [PyMC](https://docs.pymc.io/) and [NumPyro](https://num.pyro.ai/en/stable/).
* **Regularization**. Machine learning is about balancing the correct amount of regularization to avoid overfitting. Bayesian inference provides a principled way to do this, by using priors to regularize the model.
* **Uncertainty quantification**. Bayesian inference allows us to quantify the uncertainty in our predictions, and any transformation of the parameters, such as the mean, median, or quantiles, through samples from our posterior distribution. This is extremely useful in many applications, such as risk assessment, decision making, and model evaluation.

In addition, we can list some concrete use-cases of Bayesian Inference, such as:

* Censored data analysis: model the distribution of a quantity of interest, given that we only observe a subset of the data, such as in survival analysis or reliability engineering.
* A/B testing: use the posterior to quantify the probability of one variant being better than another, and to compute credible intervals for the difference in means.
* Hierarchical models: use the hyperprior to share information between groups, and improve the estimate for underrepresented groups (e.g. price elasticity of demand).
* Time series forecasting: use the prior to regularize the model and avoid overfitting, and use the posterior to quantify uncertainty in the predictions, probability of stock-outs and other quantities of interest.

```{python}
# | echo: false
import warnings

warnings.filterwarnings("ignore")
```


## Bayesian and frequentist intervals

Before we start, let's clarify the difference between two important concepts in statistics: **confidence intervals** and **credible intervals**.

If $[A, B]$ is an interval generated by a model, and $\theta$ is the parameter of interest, which of the following is bayesian and which is frequentist statement?

> **A. There is a 95% probability that the true quantity $\theta$ lies in $[A, B]$**
> 
> **B. There is 95% chance that $[A, B]$ contains the true quantity $\theta$**


::: {.callout-tip collapse="true"}
## Answer
The first is a Bayesian, the second is a frequentist statement.

The frequentist interval tells us that, if we repeat the experiment say 100 times, 95 of the constructed intervals will contain the true parameter value. The Bayesian interval tells us that, given the data we have, there is a 95% chance that the true parameter value lies in that interval. The key difference is that the bayesian credible interval conditions on the data, while the frequentist confidence interval takes into account the randomness of the data generation process on the measurements.
 
:::

One of the most intuitive explanations of the difference between both perspectives is the one presented by [Jake VanderPlas in his talk in 2014](
    https://www.youtube.com/watch?v=KhAUfqhLakw
).

Consider this simple analogy: imagine you are trying to estimate the height of a tree. The Frequentist approach would be to take a sample of trees, measure their heights, and then calculate a confidence interval based on that sample. The Bayesian approach, on the other hand, would involve taking into account your prior knowledge about the height of trees in general, and then updating that knowledge based on the sample you took. The intervals of the frequentist method would tell that this measurement procedure has 95% chance to contain the true average height of the trees, while the bayesian method would tell that, given the data we have, there is a 95% chance that the true average height of the trees lies in the interval.

```{python}
# | label: intervals
# | fig-cap: "Bayesian vs Frequentist intervals, as in [Jake VanderPlas talk in 2014](https://www.youtube.com/watch?v=KhAUfqhLakw)"
# | echo: false

import matplotlib.pyplot as plt
import numpy as np

# Set random seed for reproducibility
np.random.seed(42)

# Generate synthetic data
bayesian_params = np.random.normal(loc=0, scale=1, size=(100, 2))
frequentist_center = np.array([0, -5])
frequentist_intervals = [
    frequentist_center + np.random.normal(scale=2, size=2) for _ in range(20)
]

# Create side-by-side plots with white background
fig, (ax_bayes, ax_freq) = plt.subplots(1, 2, figsize=(12, 6))
fig.patch.set_facecolor("white")

# --- Bayesian Credible Region ---
ax_bayes.set_facecolor("white")
ax_bayes.scatter(
    bayesian_params[:, 0], bayesian_params[:, 1], color="orange", label="Parameter"
)
bayesian_ellipse = plt.Circle(
    (0, 0), 2.5, color="steelblue", alpha=0.5, label="Interval"
)
ax_bayes.add_patch(bayesian_ellipse)

ax_bayes.set_title("Bayesian Credible Region", color="black", fontsize=14)
ax_bayes.legend(loc="upper right", fontsize=10)
ax_bayes.set_xlim(-6, 6)
ax_bayes.set_ylim(-6, 6)
ax_bayes.axis("off")

# --- Frequentist Confidence Interval ---
ax_freq.set_facecolor("white")
for center in frequentist_intervals:
    ellipse = plt.Circle(center, 2.5, color="steelblue", alpha=0.3)
    ax_freq.add_patch(ellipse)
ax_freq.scatter(
    [frequentist_center[0]], [frequentist_center[1]], color="orange", label="Parameter"
)

ax_freq.set_title("Frequentist Confidence Interval", color="black", fontsize=14)
ax_freq.legend(loc="upper right", fontsize=10)
ax_freq.set_xlim(-6, 6)
ax_freq.set_ylim(-10, 2)
ax_freq.axis("off")

plt.tight_layout()
plt.show()
```

This subtle difference leads to difference answers, in some cases, and for a given question we should be careful to choose the right approach. Here are some questions that a credible interval may answer better:

| # | Bayesian | Frequentist |
|---|----------|-------------|
| 1 | “Given only three months of sales data for a brand-new SKU, how likely is it to exceed 1000 units next quarter?” | "Based on three months of data, how confident are we that we will sell more than 1000 units next quarter?" 

The frequentist communicates the confidence on the experimental procedure / model ability to cover the true value in the confidence interval. The bayesian focus on the distribution of the parameter itself.

## Basics of Bayesian Inference

In Bayesian Inference, we are
interested in finding out the distribution $P(\theta|X)$ of our parameters $\theta$,
given our data $X$. We consider that the parameters are random variables, because we always have some amount of uncertainty on their true values. 

Bayesian inference derives from the simple Bayes' rule:

$$
\underbrace{P(\theta|X)}_{\text{Posterior}} = \frac{P(X|\theta)P(\theta)}{P(X)}
$$

That, for all practical purposes, we can ignore the denominator $P(X)$, called evidence, since it is constant and does not depend on our variable of interest $\theta$. Then, it can be simplified to:

$$
 \underbrace{P(\theta|X)}_{\text{Posterior}} \propto \underbrace{P(X|\theta)}_{\text{Likelihood}} \underbrace{P(\theta)}_{\text{Prior}}
$$ {#eq-bayes-propto}

This "$\propto$" symbol indicates "proportional to", and proportionality is simpler and enough to find the posterior. We don't need to know the exact probability of $P(\theta|X)$ to estimate such distribution, as we will see, we need just to be able to answer how much a given parameter value is probable in comparison to other values. The denominator $P(X)$ serves as a normalization term, to guarantee that the posterior integrates to one,
and since it is independent of $\theta$ - the quantity we are interested in - we can just ignore it.

So, we can interpret each term as:

* Likelihood $P(X|\theta)$: probability of observing the data given parameters. The model of how the world produces data from parameters.
* Prior distribution $P(\theta)$: probability of seeing a set of parameters, not conditioned on the data. This represent our prior beliefs.
* Posterior distribution $P(\theta|X)$: probability of the parameters, given the data. 


This rule **connects** any prior belief one has about a random variable $\theta$ to what their distribution must be according to the data. Where either the prior or the likelihood is zero, the posterior probability is also zero. So the prior can also work to restrict the domain of our parameters.

Consider a simple example of having a box with a certain proportion $\theta$ of red balls. We observe a dataset $X = (K,N)$, informing the number of red balls $K$ we draw after a total trials $N$.

![A box with red and blue balls, where the proportion of red balls is $\theta$](box.png){fig-align="center" width="30%"}

* The prior distribution should reflect our initial guesses of what the share of red balls should be.
The probability of landing heads should lie between 0 and 1 ($\theta \in (0, 1)$), and a natural
choice of prior here is the [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution), that has its domain in this interval.

$$
\theta \sim Beta(\alpha, \beta)
$$
  
* The likelihood is the mathematical rule that tells us how the world produces what we see. **It’s a function of parameters, mapping them to the distribution of observations**. For sequence of draws, a natural rule to describe the number
of red balls and total draws is binomial distribution:

$$
X|\theta \sim \text{Binomial}(n, k) = \binom{n}{k} \,p^k (1-p)^{\,n-k}
$$

* The posterior will be our final belief on the values of $\theta$. It will reflect both the prior belief and the likelihood of the data.

$$
\theta|X \sim \text{Binomial}(n, k) \cdot Beta(\alpha, \beta)
$$ {#eq-bayes-box}


For some situations, we can analitically compute the posterior distribution, by replacing the likelihood and prior in Bayes' rule equation. This bernoulli-beta example is one of them. The posterior is a Beta distribution with parameters $\text{Beta}(\alpha + k, \beta + (n - k))$, where $k$ is the number of red balls observed, and $n$ is the total number of draws.

::: {.callout-tip}

The interactive view below let's you play with the prior and posterior distributions of a coin experiment. 

Note how, for $Beta(1,1)$, the prior is uniform. This means that we have no prior knowledge about the coin bias (we call this uninformative prior), and only the likelihood term will influence the posterior. 
:::

```{ojs}
//| echo: false
// Cell 1: Load jStat via dynamic import
jStat = require("jStat@latest")

// Cell 2: Interactive parameter ranges (Inputs is preloaded)
viewof alpha = Inputs.range([1, 20], {step: 1, value: 1, label: "Prior parameter Alpha (α)"});
viewof beta  = Inputs.range([1, 20], {step: 1, value: 1, label: "Prior parameter Beta (β)"});
viewof n     = Inputs.range([1, 200], {step: 1, value: 100, label: "Trials (n)"});
viewof k     = Inputs.range([0, n],    {step: 1, value: 60, label: "Red balls (k)"});

// Cell 3: Compute θ grid and densities (Plot is preloaded)
thetaValues = Array.from({length: 200}, (_, i) => i / 199);

priorDist = thetaValues.map(t => ({
  theta: t,
  density: jStat.beta.pdf(t, alpha, beta),
  type: "Prior"
}));

posteriorDist = thetaValues.map(t => ({
  theta: t,
  density: jStat.beta.pdf(t, alpha + k, beta + (n - k)),
  type: "Posterior"
}));

// Cell 4: Plot prior and posterior
Plot.plot({
  width: 600,
  height: 400,
  x: {label: "θ"},
  y: {label: "Density"},
  color: {legend: true},
  marks: [
    Plot.line(priorDist,    {x: "theta", y: "density", stroke: "type", strokeWidth: 2}),
    Plot.line(posteriorDist,{x: "theta", y: "density", stroke: "type", strokeWidth: 2})
  ]
});
```

### Hyperpriors

As in any machine learning model, we have hyperparameters in our bayesian models, that are the parameters of the prior distribution. For example, in @eq-bayes-propto, $\alpha$ and $\beta$ are two hyperparameters that we have to set before fitting the model to data. However, in Bayesian Inference, we can go further and specify *hyperpriors*, that are priors over the parameters of the priors.

Notice how Bayes' rule can be factorized using conditional probabilities:

$$
P(\theta|X) = P(X|\theta)P(\theta|\alpha,\beta)P(\alpha,\beta)
$$

so that our distribution over $\theta$ depends on the specification of the distribution of another set of parameters. You can think of this as setting priors on the hyperparameters of a model, and not on the parameters. **Bayesian Hierarchical Models** that leverage this idea, and they have powerful features that allow one to obtain better estimates than models with standard priors.

One powerful motivation of hyperpriors is when we are modelling a set of parameters that are related to each other, and we want to share information between them. For example, if we have a set of parameters $\{\theta_i\}_{i=1}^K$, that could represent the same quantity, but for $K$ different groups, we can use a hyperprior to specify our beliefs of the global distribution of these parameters, and let this distribution influence the posterior of each individual parameter $\theta_i$. This is called **hierarchical pooling**. We will see an example of this later.


## Approximating posterior from data

In other situations (most of them), developing the equations does not lead to an analytical solution.
In such cases, we can use alternative methods to approximate the posterior distribution.

There is a spectrum of methods to approximate the posterior distribution, from point estimates to sampling methods. Here are some of the most common ones, from "most bayesian" to "least bayesian":


1. **Markov Chain Monte Carlo (MCMC)**: by using the unnormalized posterior in equation @eq-bayes-propto, we can sample many instances of $\{\theta_i\}_{i=1}^K \sim p(\theta|X)$ with probabilities proportional to their true posterior probabilities. The result of the estimation is a set of samples that simulate samples from the true posterior, and can be used to compute the mean, percentile or any quantity of interest.

2. **Variational Inference (VI)**: since the true posterior $p(\theta|X)$ distribution may be hard or slow to sample from, we can use another simpler candidate distribution $q(\theta)$, and seek the parameters of this approximate distribution that minimize the distance to the true posterior. This is often done by minimizing the Kullback-Leibler divergence $KL(q||p(\cdot|X))$ using stochastic optimization algorithms. In such situation, it is called Stochastic Variational Inference (SVI). The result are parameters of the candidate distribution, that can be used to sample sets of parameters and compute the mean, percentiles or any quantity of interest. This is often faster than MCMC.

3. **Maximum A Posteriori (MAP)**: this is a point estimate of the posterior distribution, which finds the mode (maximum) of the posterior distribution $\arg \max_\theta p(\theta|X)$. It is often faster and easier to compute than the full posterior distribution, but does not provide a distribution on parameters such as VI or MCMC.



### Markov Chain Monte Carlo (MCMC)

---

![Illustration of a MCMC chain](mcmc_convergence.png){fig-align="center" width="50%"}

Estimating the full posterior distribution, and not single point, turns out to be extremely useful when we want to measure risks and other quantities. This is where MCMC comes in. MCMC is based on two key ideas: Markov Chains and Monte Carlo methods. 

Markov chains are random sequences of states $\{S_i\}$, with a given transition probability  $T(S_{i+1}|S_i)$ between them. These transitions have the mandatory property of not depending on the history of the chain, and only on the current observation. They are memory-less.

These random sequences can have an interesting property of **stationary distributions**. In other words, these sequences can have as an equilibrium a certain distribution $p$, so that, if the sequence starts at $p$, future values are also drawn according to the probability $p$. What if we could build a Markov chain that has as stationary distribution the posterior distribution we are interested in? This is the key idea of MCMC.

Then, MCMC basically designs a Markov Chain that have as stationary distribution the distribution of interest: our posterior $P(\theta|X)$.
The name Monte Carlo comes from the famous casinos in Monaco, and its basic idea is that by using random samples we can approximate some quantity of interest.

| Piece        | What it supplies                                                                 | Typical source of randomness                                       |
|--------------|----------------------------------------------------------------------------------|--------------------------------------------------------------------|
| Markov chain | A machinery for generating a sequence of states whose distribution converges to the target π(·). | Transition kernel: proposal distribution, conditional sampler, etc. |
| Monte Carlo  | The fact that, once you have (asymptotically) draws from π, you can approximate any expectation $\mathbb{E}_\pi[f(X)]$ by the empirical average $\frac{1}{N} \sum_{t=1}^N f(X_t)$. | Every random step in building and moving along the chain           |

The simplest MCMC algorithm is the **Metropolis-Hastings** algorithm, which is based on the idea of proposing new states and accepting or rejecting them based on their probabilities. 


#### Metropolis Hasting (MH) algorithm

The algorithm has two major parts: initialization and sampling.

::: {}
1. **Initialization**
    i) Choose a prior $P(\theta)$ over the parameters, and a likelihood $P(X|\theta)$ over the data.
    ii) Choose an initial value for the parameter $\theta_0$ (often sampled from the prior).
    iii) Choose a transition function $T(\theta_{t+1}|\theta_t)$ to suggest new values of $\theta$. Here we set $T(\theta_{t+1}|\theta_t) = \theta_t + \mathcal{N}(0,1)$
2. **Sampling**
   i) Propose a new candidate $\theta_{t+1} \sim T(\theta_{i+1}|\theta_{i})$
   ii) Compute $\hat{P}(\theta_{t+1}|X) = P(X|\theta_{t+1})P(\theta_{t+1})$ and $\hat{P}(\theta_{t}|X) = P(X|\theta_{t})P(\theta_{t})$
   iii) Accept the candidate with probability $\min\left(1, \frac{\hat{P}(\theta_{t+1}|X)}{\hat{P}(\theta_{t}|X)}\right)$

:::

Let's build a simple Metropolis-Hastings algorithm from scratch, to understand how it works, and try to infer the posterior distribution of proportion of red balls as in our previous example, given a sample dataset. As a prior, we will use an uniform distribution between 0 and 1, and as likelihood we will use a Bernoulli distribution with parameter $\theta$.

We first define distributions, and a simple common interface, with two methods: `log_prob` to compute the log-probability of a value, and `sample` to generate samples from the distribution. 
```{python}
# | echo: true
# | label: distributions

import numpy as np


class Distribution:
    """
    Base distribution class

    Subclasses should implement the `log_prob` and `sample` methods.
    """

    def log_prob(self, value):
        """Return the log probability of the value under the distribution."""
        raise NotImplementedError()

    def sample(self, size=1):
        """Sample from the distribution."""
        raise NotImplementedError()


class Uniform(Distribution):
    """
    Uniform distribution.

    Parameters
    ----------
    lower : float
        Lower bound of the uniform distribution.
    upper : float
        Upper bound of the uniform distribution.

    """

    def __init__(self, lower, upper):
        self.lower, self.upper = lower, upper

    def log_prob(self, value):
        value = np.asarray(value)
        probs = ((value >= self.lower) & (value <= self.upper)).astype(float) / (
            self.upper - self.lower
        )
        return np.sum(np.log(probs, where=probs > 0, out=np.full_like(probs, -np.inf)))

    def sample(self, size=1):
        return np.random.uniform(self.lower, self.upper, size=size)


class Bernoulli(Distribution):
    """
    Bernoulli distribution.

    Parameters
    ----------
    p : float
        Probability of success (1) in the Bernoulli distribution.
        Must be in the range [0, 1].
    """

    def __init__(self, p):
        self.p = p

    def log_prob(self, value):
        value = np.asarray(value)
        return np.sum(value * np.log(self.p) + (1 - value) * np.log(1 - self.p))

    def sample(self, size=1):
        return np.random.binomial(1, self.p, size=size)
```

Now, we implement the Metropolis-Hastings algorithm, which will use these distributions to sample from the posterior.

```{python}

class MetropolisHasting:
    """
    A simple Metropolis Hasting algorithm

    Parameters
    ----------
    num_samples : int
        Number of samples to draw from the posterior.
    num_warmup : int
        Number of warmup iterations to discard before starting to collect samples.
    step_size : float
        Step size for the proposal distribution. This controls how far the proposed samples can be from the current sample.
    """
    def __init__(self, num_samples, num_warmup, step_size=0.1):
        self.num_samples, self.num_warmup, self.step_size = (
            num_samples,
            num_warmup,
            step_size,
        )

    def _transition(self, x):
        """
        Samples $x_{i+1}$ from transition function.
        
        Parameters
        ----------
        x : float
            Current sample value, $x_t$
        """
        return x + np.random.normal(0, self.step_size)

    def run(self, data, prior, likelihood, p0=None):
        """
        Run the MH Algorithm.

        Parameters
        ----------
        data : array-like
            Observed data to compute the likelihood.
        prior : Distribution
            Prior distribution of the parameter.
        likelihood : Distribution
            Likelihood distribution of the data given the parameter.
        p0 : float, optional
            Initial value for the parameter. If None, it will sample from the prior.
        
        Returns
        -------
        samples : np.ndarray
            Samples from the posterior distribution.
        accepted : np.ndarray
            Array indicating whether each sample was accepted (1) or rejected (0).
        """
        N = self.num_samples + self.num_warmup
        samples = np.full(N, np.nan)
        accepted = np.zeros(N, dtype=int)
        cur = prior.sample()[0] if p0 is None else p0
        samples[0] = cur
        for i in range(1, N):
            cand = self._transition(cur)
            logp_cur = prior.log_prob(cur) + likelihood(cur).log_prob(data)
            logp_cand = prior.log_prob(cand) + likelihood(cand).log_prob(data)
            if np.log(np.random.uniform()) <= logp_cand - logp_cur:
                cur = cand
                if i >= self.num_warmup:
                    accepted[i] = 1
            samples[i] = cur
        return samples, accepted


# ------------------------------------------------------------------
# Run the sampler
# ------------------------------------------------------------------
np.random.seed(42)
data = np.random.binomial(1, 0.2, size=50)
mh = MetropolisHasting(num_samples=250, num_warmup=25, step_size=0.1)
samples, accept = mh.run(data, prior=Uniform(0, 1), likelihood=Bernoulli, p0=0.6)

```

```{python}
# | echo: False
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from IPython.display import HTML, display

col_rej = "red"
col_acc = "blue"

# ------------------------------------------------------------------
# One figure, two axes, shared y‐axis
# ------------------------------------------------------------------
fig, (ax_trace, ax_hist) = plt.subplots(ncols=2, sharey=True, figsize=(12, 5))

# Trace axis
ax_trace.set_xlim(0, len(samples))
ax_trace.set_ylim(0, 1)
ax_trace.set_xlabel("Iteration")
ax_trace.set_ylabel("Sample value")
ax_trace.set_title("Trace (accepted vs rejected)")
sc_rej = ax_trace.plot(
    [], [], marker="x", linestyle="none", color=col_rej, alpha=0.4, label="Rejected"
)[0]
sc_acc = ax_trace.plot(
    [], [], marker="o", linestyle="none", color=col_acc, alpha=0.6, label="Accepted"
)[0]
ax_trace.axhline(
    0.2, linestyle="--", linewidth=1, color="black", alpha=0.7, label="Ground-truth p"
)
ax_trace.legend()

# Histogram axis
ax_hist.set_xlabel("Frequency")
ax_hist.set_title("Histogram up to current step")
ax_hist.set_xlim(0, None)  # autoscale x‐axis
# y‐limits inherited from sharey


def init():
    sc_rej.set_data([], [])
    sc_acc.set_data([], [])
    ax_hist.cla()
    ax_hist.set_xlabel("Frequency")
    ax_hist.set_title("Histogram up to current step")
    return sc_rej, sc_acc


def update(frame):
    xs = np.arange(frame + 1)
    ys = samples[: frame + 1]
    mask = accept[: frame + 1].astype(bool)

    # trace update
    sc_acc.set_data(xs[mask], ys[mask])
    sc_rej.set_data(xs[~mask], ys[~mask])

    # histogram update
    ax_hist.cla()
    ax_hist.set_xlabel("Frequency")
    ax_hist.set_ylim(0, 1)
    ax_hist.set_title("Histogram up to current step")
    ax_hist.hist(
        ys[mask],
        bins=10,
        orientation="horizontal",
        alpha=0.6,
        density=True,
        color=col_acc,
        label="Accepted",
    )

    ax_hist.legend(loc="upper right")

    return sc_rej, sc_acc, *ax_hist.patches


anim = FuncAnimation(
    fig, update, init_func=init, frames=len(samples), interval=5, blit=False
)

# Display inline
# display(HTML(anim.to_jshtml()))


# Optional: save as GIF
anim.save("mh_accepted_rejected_fast.gif", writer="pillow", fps=30)
plt.close()

```

![Metropolis Hasting. The plot on the left shows the MCMC path, with accepted (blue) and rejected (red) samples. The plot on the right illustrates an histogram of accepted samples.](mh_accepted_rejected_fast.gif)

We can see how, after a certain number of iterations, the samples start to cluster around the true value. Other MCMC algorithms, such as Hamiltonian Monte Carlo, leverage the gradients of the posterior to propose new samples, and can converge faster than the simple MCMC algorithm we just implemented. Libraries such as [PyMC](https://docs.pymc.io/) and [NumPyro](https://num.pyro.ai/en/stable/) provide efficient implementations of MCMC algorithms, and can be used to sample from the posterior distribution of more complex models.

#### Simple numpyro implementation

In numpyro, things are much simpler and, thanks to jax-based backend, also faster. We need to define a function that receives our data, and defines the relationship between variable samples (observed and not observed).

```{python}
import jax
import jax.numpy as jnp
from jax import random
import numpyro
import numpyro.distributions as dist
from numpyro.infer import NUTS, MCMC


def bernoulli_model(data=None):
    """Uniform-prior Bernoulli model p ~ U(0,1)."""
    p = numpyro.sample("p", dist.Uniform(0.0, 1.0))
    with numpyro.plate("obs", data.shape[0]):
        numpyro.sample("y", dist.Bernoulli(probs=p), obs=data)
```

Now, we can use the `NUTS` kernel to sample from the posterior distribution. NUTS is a variant of Hamiltonian Monte Carlo that automatically tunes the step size and number of leapfrog steps, making it more efficient than the basic Metropolis-Hastings algorithm.

```{python}
# Random seed for reproducibility
key = jax.random.PRNGKey(0)

nuts_kernel = NUTS(
    bernoulli_model, step_size=0.1
)  # step_size matches your MH proposal scale
mcmc = MCMC(nuts_kernel, num_warmup=200, num_samples=1000, progress_bar=False)
mcmc.run(key, data=data)
posterior = mcmc.get_samples()["p"]  # (250,) array – exactly like your `samples`
```

```{python}
# | echo: false
# Plot histogram of posterior samples
plt.hist(
    posterior, bins=30, density=True, alpha=0.7, color="blue", label="Posterior samples"
)
plt.axvline(posterior.mean(), color="red", linestyle="--", label="Posterior mean")
plt.axvline(
    jnp.quantile(posterior, 0.025), color="green", linestyle="--", label="2.5% quantile"
)
plt.axvline(
    jnp.quantile(posterior, 0.975),
    color="green",
    linestyle="--",
    label="97.5% quantile",
)
plt.axvline(0.2, color="black", linestyle="-", label="True p = 0.2")
plt.xlabel("p")
plt.ylabel("Density")
plt.title("Posterior Distribution of p")
plt.legend()
plt.show()

print("Posterior mean of p:", posterior.mean())
print("95% credible interval:", jnp.quantile(posterior, jnp.array([0.025, 0.975])))
```

One downside of MCMC is that it can be slow to converge, especially for high-dimensional problems. Other approaches provide an approximation of the posterior distribution, with the objective of being faster. One of such methods is Variational Inference.

### Variational Inference (VI)



```{python}
# | echo: False

# Warning, this is not a true VI!
from IPython.display import HTML
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from scipy.stats import beta as beta_dist

# --------------------------------------------------
# 1.  True posterior:  p(θ | y) = Beta(2, 5)
# --------------------------------------------------
alpha_true, beta_true = 2.0, 5.0
theta = np.linspace(0, 1, 400)
posterior = beta_dist.pdf(theta, alpha_true, beta_true)
posterior /= posterior.max()  # scale peak to 1 for tidy overlay

# --------------------------------------------------
# 2.  Variational family:  q(θ) = Beta(α, β)  (start broad & flat)
# --------------------------------------------------
alpha_q, beta_q = 0.5, 0.5  # initial guess
lr = 0.10  # learning-rate-ish factor
n_iter = 60  # animation frames

# --------------------------------------------------
# 3.  Matplotlib figure and empty artists
# --------------------------------------------------
fig, ax = plt.subplots()
(line_post,) = ax.plot(theta, posterior, lw=2, label="True Beta(2, 5)")
(line_q,) = ax.plot([], [], lw=2, label="Variational q(θ)")
(point,) = ax.plot([], [], "ro", label="q mean")


def init():
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1.05)
    ax.set_xlabel(r"$\theta$")
    ax.set_ylabel("Density (scaled)")
    ax.set_title("Variational inference: q(θ)=Beta(α,β) → Beta(2,5)")
    ax.legend()
    return line_post, line_q, point


# --------------------------------------------------
# 4.  Update rule (simple exponential approach)
# --------------------------------------------------
def update(frame):
    global alpha_q, beta_q

    # toy “natural-gradient” step: move a fraction toward the target α*, β*
    alpha_q += lr * (alpha_true - alpha_q)
    beta_q += lr * (beta_true - beta_q)

    # recompute q density
    q_density = beta_dist.pdf(theta, alpha_q, beta_q)
    q_density /= q_density.max()  # scale for overlay
    line_q.set_data(theta, q_density)

    # mark the variational mean
    mean_q = alpha_q / (alpha_q + beta_q)
    point.set_data([mean_q], [0])  # at baseline for clarity

    ax.set_title(f"Iter {frame+1}/{n_iter}   α={alpha_q:.2f}, β={beta_q:.2f}")
    return line_q, point


ani = FuncAnimation(
    fig,
    update,
    frames=n_iter,
    init_func=init,
    interval=100,  # ms between frames
    blit=True,
    repeat=False,
)
plt.close()  # prevent duplicate static plot
HTML(ani.to_jshtml())
```

In Variational Inference (VI), we are not interested in the exact posterior distribution. Instead, we want to simplify and find an approximation by using another distribution $q$, called variational distribution, belonging to a family of tractable distributions such as Normal distribution.

The goal is to find $q(\theta)$ that is as close as possible to the true posterior $p(\theta|X)$, in the sense of minimizing the Kullback-Leibler divergence:

$$
\arg\min_{q \in \mathcal{Q}} KL(q||p(\cdot|X))
$$

where $\mathcal{Q}$ is the family of distributions we are considering for $q(\theta)$. 


```{python}
# | echo: false
# | fig-align: "center"

import matplotlib.patches as patches

# Create figure and axis
fig, ax = plt.subplots(figsize=(8, 4))

# Draw the "space of all distributions"
ax.set_xlim(0, 10)
ax.set_ylim(0, 10)
ax.set_aspect("equal")
ax.set_facecolor("#f5f5f5")
ax.text(5, 9.5, "Space of all distributions", ha="center", fontsize=14)

# Draw the variational family as an ellipse
ellipse = patches.Ellipse(
    (4, 5), width=5, height=3, edgecolor="black", facecolor="lightgrey", alpha=0.5
)
ax.add_patch(ellipse)
ax.text(
    4, 5.5, "Variational\nfamily $\mathcal{Q}$", ha="center", va="center", fontsize=12
)

# Plot the true posterior p(z|x)
ax.plot(8, 5, "o", color="brown", markersize=10)
ax.text(8.3, 5, r"$p(\theta \mid X)$", fontsize=12, va="center")

# Plot the optimal q*(z) on the ellipse edge
edge_x = 4 + (5 / 2)  # center x + horizontal radius
edge_y = 5  # same center y
ax.plot(edge_x, edge_y, "o", color="black", markersize=10)
ax.text(edge_x + 0.3, edge_y, r"$q(\theta)$", fontsize=12, va="center")

# Show plot borders by enabling spines and turning off ticks
ax.spines["top"].set_visible(True)
ax.spines["right"].set_visible(True)
ax.spines["bottom"].set_visible(True)
ax.spines["left"].set_visible(True)
ax.set_xticks([])
ax.set_yticks([])

plt.tight_layout()
plt.show()
```


In the past, we would
choose $\mathcal{Q}$ so that an analitical solution would be available, but this is not the case anymore. Modern probabilistic programming libraries come with the feature of Stochastic Variational Inference (SVI) that can handle arbitrary families of distributions $\mathcal{Q}$ to find a numerical solution. The trick is having $q_\lambda$ defined by a set of parameters $\lambda$, which are directly optimized.

In Numpyro, the variational distributions are called **guides**, and there are tools for automatically performing a SVI of our model. For our example of Bernoulli distribution, it would look like this.

We first define our SVI engine, with the help of a guide (`AutoNormal`, that uses a normal distribution as variational family, and the parameters $\lambda = \{\mu, \sigma\}$ of mean and standard deviation are optimized), and an optimizer.
```{python}
from numpyro.infer import SVI, Trace_ELBO, autoguide
from numpyro.optim import Adam


guide = autoguide.AutoNormal(bernoulli_model)  # guide with diagonal covariance
optimizer = Adam(1e-2)  # any optax optimizer works


svi = SVI(bernoulli_model, guide, optimizer, loss=Trace_ELBO())
```

And now we can run the optimization loop, by initing the state of the SVI and sequentially updating it.

```{python}
n_iterations = 5000
# Run the optimization loop
# Initialize SVI state
rng_key = jax.random.PRNGKey(0)
svi_state = svi.init(rng_key, data)


# Compile the update function for efficiency
@jax.jit
def run_step(state, data):
    return svi.update(state, data)


losses = []
for i in range(n_iterations):
    svi_state, loss = run_step(svi_state, data)
    losses.append(loss)
    if i % 500 == 0:
        print(f"Iteration {i}, ELBO Loss = {loss:.4f}")

plt.figure(figsize=(8,4))
plt.plot(losses, label="ELBO Loss")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.title("ELBO Loss during Variational Inference")
plt.legend()
plt.show()
```

The solution looks like below:

```{python}
# | code-fold: true

# Extract optimized parameters and generate posterior samples
params = svi.get_params(svi_state)
posterior_samples = guide.sample_posterior(
    random.PRNGKey(1), params, sample_shape=(1000,)
)

# Plot the posterior samples
plt.hist(
    posterior_samples["p"],
    bins=30,
    density=True,
    alpha=0.7,
    color="blue",
    label="Posterior samples",
)
plt.axvline(
    posterior_samples["p"].mean(), color="red", linestyle="--", label="Posterior mean"
)
# True value
plt.axvline(0.2, color="black", linestyle="-", label="True p = 0.2")
plt.xlabel("p")
plt.ylabel("Density")
plt.title("Posterior Distribution of p (Variational Inference)")
plt.legend()
plt.show()
```


### Maximum A Posteriori (MAP)

The Maximum A Posteriori (MAP) estimate is the mode of the posterior distribution: the value of $\theta$ that maximizes the posterior probability density function. Because it does not compute the entire distribution over $\theta$, and just the most probable value according to the posterior, it is often faster and easier to compute than the full posterior distribution.


```{python}
# | echo: False
from IPython.display import HTML
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

# Define θ range
theta = np.linspace(-5, 5, 500)
prior = np.exp(-0.5 * (theta) ** 2)  # Standard normal prior
likelihood = np.exp(-0.5 * (theta - 2) ** 2 / 0.5**2)  # Likelihood centered at θ=2

posterior = prior * likelihood  # Unnormalized posterior

# Normalize for visualization
prior /= np.max(prior)
likelihood /= np.max(likelihood)
posterior /= np.max(posterior)

fig, ax = plt.subplots()
(line_prior,) = ax.plot([], [], label="Prior")
(line_likelihood,) = ax.plot([], [], label="Likelihood")
(line_posterior,) = ax.plot([], [], label="Posterior (unnormalized)")
(point,) = ax.plot([], [], "ro", label="MAP estimate")


def init():
    ax.set_xlim(-5, 5)
    ax.set_ylim(0, 1.2)
    ax.legend()
    return line_prior, line_likelihood, line_posterior, point


def update(frame):
    if frame == 0:
        line_prior.set_data(theta, prior)
        line_likelihood.set_data([], [])
        line_posterior.set_data([], [])
        point.set_data([], [])
    elif frame == 1:
        line_likelihood.set_data(theta, likelihood)
    elif frame == 2:
        line_posterior.set_data(theta, posterior)
    elif frame == 3:
        map_estimate = theta[np.argmax(posterior)]
        map_value = posterior.max()
        point.set_data([map_estimate], [map_value])
    return line_prior, line_likelihood, line_posterior, point


ani = FuncAnimation(fig, update, frames=4, init_func=init, blit=True, repeat=False)

plt.close()
HTML(ani.to_jshtml())
```

Although not a "true" bayesian inferece, MAP can provide us point estimates that balance the prior and likelihood. The posterior $P(\theta|X)$  is treated as a function whose maximum we want to find, and
due to amazing libraries such as numpyro, and jax, we
can leverage autodiff to compute the gradient of the posterior, and rapidly estimate it. The Maximum Likelihood Estimate (MLE) would be a frequentist approach similar to MAP, but, in this case, we completely ignore the prior, and optimize the likelihood function. In this sense, you can think of
MAP as a regularized version of MLE, where the prior acts as a regularization term.


It is really simple to implement MAP it in numpyro, take a look below. We use a guide, that converts our sample sites to point estimates, and then use the `SVI` (Stochastic Variational Inference) class to optimize the parameters of the guide. The guide is a simple point estimate, so it collapses to a single value for each parameter.


```{python}

guide = autoguide.AutoDelta(bernoulli_model)  # guide with point estimates
svi = SVI(bernoulli_model, guide, Adam(0.01), loss=Trace_ELBO())

# Init and optimize
rng_key = jax.random.PRNGKey(0)
svi_state = svi.init(rng_key, data)

losses = []
for i in range(n_iterations):
    svi_state, loss = run_step(svi_state, data)
    losses.append(loss)
    if i % 500 == 0:
        print(f"Iteration {i}, ELBO Loss = {loss:.4f}")

plt.figure(figsize=(8, 4))
plt.plot(losses, label="ELBO Loss")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.title("ELBO Loss during Variational Inference")
plt.legend()
plt.show()

# Extract the MAP estimate (a scalar)
params = svi.get_params(svi_state)
posterior = guide.sample_posterior(random.PRNGKey(1), params, sample_shape=())
print("MAP estimate for p:", float(posterior["p"]))
```


::: {.callout-tip}

## Don't be afraid of priors! You may be already using them :)

In many cases, **the MAP estimate is equivalent to the MLE estimate with a regularization term!** For example, the famous Ridge and Lasso regressions, that penalize coefficients far from zero, can be seen as MAP estimates with Gaussian and Laplace priors centered on zero, respectively. As the parameter value increases in magnitude, the prior reduces the posterior probability, effectively penalizing large coefficients. This implies that another way to see such regression methods in application of your prior beliefs about the parameters being close to zero.

:::


## Conclusion


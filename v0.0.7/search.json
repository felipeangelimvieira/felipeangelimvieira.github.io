[
  {
    "objectID": "posts/code_practices/index.html",
    "href": "posts/code_practices/index.html",
    "title": "Can your code pass the test set?",
    "section": "",
    "text": "Code is not meant to be written once and never touched again. It is meant to be read, understood, and modified over time. This is especially true in dynamic environments, where requirements change, new features are added, and bugs are fixed. In such environments, code design quality is not just a nice-to-have; it is essential for the long-term success of the project.\nHere, I wish to share some thoughts on code design qualities, focusing on what we want, not how to get there. This is not a guide on how to write good code but rather a reflection on what good code design means in the context of software engineering and data science. This can also be useful for managers, product owners, and stakeholders who do not code but want to understand how quality impacts the product they are building.\nThe key takeaway I wish you to have is that good code design is about resilience. It is about not overfitting to our myopic view of the present, but rather about crafting a solution that can adapt and generalize to future requirements, just like a good machine learning model. It is about how effortless it is to adapt to changes."
  },
  {
    "objectID": "posts/code_practices/index.html#the-map-is-not-the-territory",
    "href": "posts/code_practices/index.html#the-map-is-not-the-territory",
    "title": "Can your code pass the test set?",
    "section": "The map is not the territory",
    "text": "The map is not the territory\n\n\n\n\n\nThere is this nice quote from Alfred Korzybski that says “the map is not the territory”. This means that any representation of reality is not reality itself, but rather a simplification of it. Also, it is shaped by our biases and limitations.\nBringing it to the context of this post, we can say that product requirements are not the product we are building. They are a simplification of what we want to build. Naturally, details can be missing and assumptions can be wrong, because sometimes reality is more complex than what we can express in words. Sometimes, we just don’t know, and that is okay. We just have to be cautious not to mistake the empty with the void.\nThis idea is key to understanding what comes next. Our code is designed to solve problems, but our definitions of problems are not perfect. They are made by humans, and hence this human aspect and the imprecise understanding of what should be done is deeply connected to the quality of the code we write, even more so than what our linter and unit tests end up being."
  },
  {
    "objectID": "posts/code_practices/index.html#easy-is-not-simple",
    "href": "posts/code_practices/index.html#easy-is-not-simple",
    "title": "Can your code pass the test set?",
    "section": "Easy is not simple",
    "text": "Easy is not simple\nBefore using the word “simple” to describe code, we should properly define what we mean by that. As gloriously explained by Rich Hickey in his talk “Simplicity Matters,” simple is not the same as easy. Simple is about not packing together unrelated concerns; it is about giving clear responsibility to each component, and making it easy to modify and extend. Easy, on the other hand, is subjective; it is what is closer to your knowledge and experience, it is what you are used to doing, and not necessarily good. We should focus here on the code, not on our perspective of knowledge and experience.\nThis definition aligns with the one in the book “A Philosophy of Software Design” (Ousterhout (2018)), which is a huge inspiration and reference for this post.\n\nComplexity is anything related to the structure of a software system that makes it hard to understand and modify the system\n\nComplexity is not related to the number of lines or functions; it is about the bigger picture. In his book, John Ousterhout also enumerates some symptoms of complexity:\n\nChange amplification: when a change in one part of the system requires changes in many other parts.\nCognitive load: when the code is hard to understand, requiring a lot of mental effort to comprehend.\nUnknown unknowns: when it is hard to diagnose where a change is needed to complete a task.\n\nBasically, simplicity implies less effort in the long term.\nThink about the code as a graph. The nodes are components (e.g., functions) we create, and the edges represent their dependencies. Simple code would be a graph with few edges, where edges represent dependencies of components on others. A dependency can be, for example, an assumption of the structure of the output of one component, the existence of certain methods, etc. The number of edges would be proportional to the complexity of the system and our cognitive load.\n\n\n\n\n\n\n\n\n\nIf we would perturb the graph above, such as changing the output of node 11, it would only affect the nodes that are directly connected to it, and not the whole graph. This is what we want to achieve with our code: a graph with few edges, where we can easily replace parts without affecting the whole system."
  },
  {
    "objectID": "posts/code_practices/index.html#the-objective-function",
    "href": "posts/code_practices/index.html#the-objective-function",
    "title": "Can your code pass the test set?",
    "section": "The objective function",
    "text": "The objective function\nHere, I’ll try to define a conceptual “objective function”—not mathematical, but rather a guiding principle that should shape how we design and structure our code.\n\nOverfitting to training data\nAt first glance, you might think that code is good if it simply does what it’s supposed to do. However, this perspective often encourages quick-and-dirty solutions and spaghetti code that might solve immediate problems but overlook the bigger picture.\n\n\n\n\n\nImagine an abstract space of requirements, like the one above. We can only see a tiny fraction of our long-term goals clearly. Focusing too narrowly on the present moment—what we already know—makes us overfit to our current understanding, limiting our ability to adapt to new challenges. The more complex and specifically tuned code is, the more likely it is to overfit and struggle when things inevitably change.\n\n\n\n\n\nWhile “easy” solutions may feel quicker and simpler initially, generalizable, well-designed solutions usually save more effort in the long run. They avoid the exponential increase in complexity and maintenance costs that come from constantly adapting overfitted solutions.\n\n\n\n\n\n\n\n\n\n\n\nThe regret\nI’ve often wondered what exactly we’re optimizing for when designing and writing code. Just like machine learning models use metrics to evaluate their performance, we need something to measure code quality. After some thought, I’ve come to see regret as a useful measure.\nIn simple terms, regret is the extra effort spent compared to an ideal solution. This concept is borrowed from machine learning, particularly from the “multi-armed bandit” problem, where decisions must be made based on limited information. The goal there is to minimize regret—the gap between your choices and the best possible outcomes—over time. This mirrors how we write code, making choices with incomplete information and constantly adapting as we learn more.\nExperienced developers naturally have lower regret because they anticipate changes more effectively, ask better questions early on, and build resilient systems from the start.\n\n\n\n\n\n\n\n\n\n\n\nGood code starts before writing code\nSince we rarely have full information upfront, we shouldn’t aim for perfect code immediately. Instead, the first thing we should do is ask questions:\n\nWhat requirements are likely to change?\nWhat requirements are unlikely to change?\nWhat assumptions are we making?\nWhat is the expected lifespan of the product?\n\nWith the answers to these questions, we can start shaping our design. Once we have a draft proposal, we can evaluate how different possible future changes would affect the current structure.\n\n\n\n\n\nDesigning often involves identifying stable components and defining contracts between them. Asking the right questions early helps us group functionalities meaningfully and build systems that remain flexible over time.\n\n\nAvoid premature optimization\nWhile it’s important to anticipate change, guessing too far ahead about possible future requirements can be dangerous. This is known as premature optimization. It can create significant regret—first due to excessive effort upfront, and later because reality rarely matches our early guesses.\n\n\n\n\n\n\n\n\n\nI once found myself in this trap: trying to optimize a codebase that wasn’t even in production yet. I went deep into SOLID principles, endlessly splitting functions and abstracting for flexibility I thought I might need later. But those future requirements never arrived. The code became so abstract that it was hard to maintain, and when real changes eventually came, I had to rewrite everything anyway."
  },
  {
    "objectID": "posts/code_practices/index.html#conclusion",
    "href": "posts/code_practices/index.html#conclusion",
    "title": "Can your code pass the test set?",
    "section": "Conclusion",
    "text": "Conclusion\nWriting good code isn’t just about making something that works today — it’s about building solutions that can adapt, evolve, and scale as requirements inevitably change. Much like training a machine learning model, if we overfit our code to the present, we may pay a heavy price in the future. Balancing simplicity, flexibility, and pragmatism is what leads to resilient systems with low long-term maintenance cost.\nBy focusing on minimizing regret—the extra effort caused by early design decisions that don’t age well—we can make smarter choices even when information is incomplete. And while experience helps us make better bets, asking the right questions early on dramatically increases our odds of success.\nIn the end, it’s not about writing perfect code — it’s about writing code that can gracefully survive its own future."
  },
  {
    "objectID": "posts/code_practices/index.html#key-takeaways",
    "href": "posts/code_practices/index.html#key-takeaways",
    "title": "Can your code pass the test set?",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nCode quality is about resilience, not just correctness\nOverfitting code to today’s requirements creates fragile systems.\nMinimize regret: optimize for adaptability, not hypothetical future needs.\nGood design starts with asking the right questions, not jumping into abstractions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hey there. Here I talk about science, data, and other things. Opinions expressed are my own.\nTech Lead @ Mercado Livre, Core Dev @ sktime (pro bono) and developer/creator of Prophetverse."
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Bayesian Inference - Introduction with Applications\n\n\n\n\n\n\n\n\nFelipe Angelim\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Bytes of Thoughts",
    "section": "",
    "text": "Can your code pass the test set?\n\n\n\nsoftware design\n\nmachine learning\n\n\n\n\n\n\n\n\n\nJun 10, 2025\n\n\nFelipe Angelim\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#about-me",
    "href": "presentations/bayesian_inference/index.html#about-me",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "About me",
    "text": "About me\n\n\n\n\n\n\n\n\n\nFelipe Angelim\n\n\nTech Lead @ Mercado Libre\n\n\nCore Dev @ Sktime\n\n\nCreator/Dev @ Prophetverse\n\n\nfelipeangelim.com"
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#agenda",
    "href": "presentations/bayesian_inference/index.html#agenda",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Agenda",
    "text": "Agenda\n\nMotivation\nBayes: Priors, posteriors, and likelihoods\nBayesian Inference applications"
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#motivation-1",
    "href": "presentations/bayesian_inference/index.html#motivation-1",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Motivation",
    "text": "Motivation"
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#motivation-2",
    "href": "presentations/bayesian_inference/index.html#motivation-2",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Motivation",
    "text": "Motivation\n\n\nScalar number is not enough: we want to quantify uncertainty.\nSmall data requires regularization: with bayesian methods, we use priors and domain knowledge."
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#key-ingredients",
    "href": "presentations/bayesian_inference/index.html#key-ingredients",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Key ingredients",
    "text": "Key ingredients\nPriors, Posteriors, and Likelihoods\n\\[\n\\underbrace{P(\\theta|X)}_{\\text{Posterior}} = \\frac{\\overbrace{P(X|\\theta)}^{\\text{Likelihood}} \\; \\overbrace{P(\\theta)}^{\\text{Prior}}}{\\underbrace{P(X)}_{\\text{Evidence}}} \\implies \\quad \\quad P(\\theta|X) {\\LARGE \\propto} P(X|\\theta) P(\\theta)\n\\]"
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#inference",
    "href": "presentations/bayesian_inference/index.html#inference",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Inference",
    "text": "Inference\n\nWe only now how to compute \\(f_X(\\theta) = P(X|\\theta)P(\\theta)\\), that is proportional to the posterior.\nInference methods:\n\nSampling methods: e.g., Markov Chain Monte Carlo (MCMC).\nVariational inference: approximate the posterior with a simpler distribution.\nMaximum a Posteriori (MAP): find the mode of the posterior distribution."
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#inference-1",
    "href": "presentations/bayesian_inference/index.html#inference-1",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Inference",
    "text": "Inference\nMarkov Chain Monte Carlo (MCMC)\n\n\n\nMarkov Chains: sequence of random events/states, with transition probabilities between them.\nCan have stationary distributions, “equilibrium”.\nIdea: build a chain whose “equilibrium” is the posterior distribution of interest.\nHere Monte Carlo is used to be able to sample from such target distribution."
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#inference-2",
    "href": "presentations/bayesian_inference/index.html#inference-2",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Inference",
    "text": "Inference\nMarkov Chain Monte Carlo (MCMC)"
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#simple-linear-regression",
    "href": "presentations/bayesian_inference/index.html#simple-linear-regression",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\n\n\\[\n\\alpha \\sim N(0, 10) \\\\\n\\beta \\sim N(0, 10) \\\\\n\\sigma \\sim \\text{HalfCauchy}(5) \\\\\nY_i \\sim N(\\alpha + \\beta X_i, \\sigma)\n\\]\nimport numpyro\nfrom numpyro import distributions as dist\n\ndef linear_regression_model(x, y=None):\n    \n    # Priors on intercept and slope\n    alpha = numpyro.sample(\"alpha\", dist.Normal(0.0, 10.0))\n    beta = numpyro.sample(\"beta\", dist.Normal(0.0, 10.0))\n\n    # Prior on noise scale (sigma &gt; 0)\n    sigma = numpyro.sample(\"sigma\", dist.HalfCauchy(5.0))\n\n    # Likelihood\n    mean = alpha + beta * x\n    with numpyro.plate(\"data\", x.shape[0]):\n        numpyro.sample(\"obs\", dist.Normal(mean, sigma), obs=y)"
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#simple-linear-regression-1",
    "href": "presentations/bayesian_inference/index.html#simple-linear-regression-1",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\n\nfrom numpyro.infer import MCMC, NUTS\nimport jax.random as random\n\n# Random key for reproducibility\nrng_key = random.PRNGKey(0)\n\nkernel = NUTS(linear_regression_model)\nmcmc = MCMC(kernel, num_warmup=1000, num_samples=2000)\nmcmc.run(rng_key, x_data, y_data)\nposterior_samples = mcmc.get_samples()"
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#bayesian-neural-networks",
    "href": "presentations/bayesian_inference/index.html#bayesian-neural-networks",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Bayesian Neural Networks",
    "text": "Bayesian Neural Networks\n\n\n\nExtend this idea of “random” parameters to neural networks’ weights.\nUse priors on weights, and sample from the posterior distribution.\n\\(weights \\sim N(0, I)\\), for example\n\n\n\n\nBlundell, Charles, et al. “Weight uncertainty in neural network.” International conference on machine learning. PMLR, 2015."
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#inference-3",
    "href": "presentations/bayesian_inference/index.html#inference-3",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Inference",
    "text": "Inference\nVariational Inference\n\nWe can also accept that the true posterior may be really hard to compute, and approximate it with a simpler distribution.\nSearch \\(q \\in Q\\) that minimizes the Kullback-Leibler divergence \\(D_{KL}(q || p(\\cdot | X))\\) w.r.t the true posterior.\nVariational inference (VI) provides a fast and approximate solution to the problem.\nNowadays, libraries provide automatic Stochastic Variational Inference (SVI).\nUsually the choice for Bayesian Neural Networks and large datasets."
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#inference-4",
    "href": "presentations/bayesian_inference/index.html#inference-4",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Inference",
    "text": "Inference\nMaximum A Posteriori (MAP)\n\nFind the mode (maximum) of the posterior distribution.\nNot a full Bayesian inference, but can be useful for fast inference and regularization.\n\n\\[\n\\arg\\max_\\theta P(\\theta | X)\n\\]"
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#inference-5",
    "href": "presentations/bayesian_inference/index.html#inference-5",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Inference",
    "text": "Inference\nMaximum A Posteriori (MAP)\n\nYou are already using bayes!\nRidge regression, and Lasso, can be obtained from a Bayesian perspective!\nBayesian inference offer a more interpretable vision over regularization.\n\n\n\nRidge regression:\n\\[\nY|X = X\\beta + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2 I) \\\\\n\\hat{\\beta} = \\arg\\min_\\beta \\left\\{ ||y - X\\beta||^2 + \\lambda ||\\beta||^2 \\right\\}\n\\]\n\nBayesian Ridge regression:\n\\[\nY | X \\sim N(X\\beta, \\sigma^2 I) \\\\\n\\beta \\sim N(0, \\frac{I}{\\tau^2}) \\\\\n\\hat{\\beta} = \\arg\\max_\\beta P(\\beta | X, Y)\n\\]"
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#you-are-already-using-bayes",
    "href": "presentations/bayesian_inference/index.html#you-are-already-using-bayes",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "You are already using bayes",
    "text": "You are already using bayes\n\\[\nP(\\beta)\n\\]"
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#total-addressable-market-tam-estimation",
    "href": "presentations/bayesian_inference/index.html#total-addressable-market-tam-estimation",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Total Addressable market (TAM) estimation",
    "text": "Total Addressable market (TAM) estimation\n\nProblem: Estimate the total number of potential users/customers in a market.\nApproach: Use a Bayesian model to estimate the growth of users over time, incorporating prior knowledge about market saturation.\n\n\n\n\n\n\\[\nG(t) = \\frac{C_1(t-t_0) + C_2}{\\left(1 + \\exp(-\\alpha v (t - t_0))\\right)^{\\frac{1}{v}}}\n\\]\n\\[\nC_2 \\in \\mathbb{R}_+ = \\text{is the constant capacity term}\\\\\nC_1 \\in \\mathbb{R}_+ = \\text{is the linear increasing rate of the capacity}\\\\\nt_0 \\in \\mathbb{R} = \\text{is the time offset term}\\\\\nv \\in \\mathbb{R}_+ = \\text{determines the shape of the curve} \\\\\n\\alpha \\in \\mathbb{R} = \\text{is the rate}\n\\]\n\n\nSee more in custom trend tutorial in prophetverse"
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#marketing-mix-modeling",
    "href": "presentations/bayesian_inference/index.html#marketing-mix-modeling",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Marketing Mix Modeling",
    "text": "Marketing Mix Modeling\n\nProblem: Estimate the impact of different marketing channels on sales.\nApproach: Use Bayesian models to:\n\nIncorporate prior knowledge about the effectiveness of each channel.\nEstimate the posterior distribution of the impact of each channel on sales.\nIncorporate A/B testing results to refine estimates.\n\n\n\n\\[\nE[\\text{Sales} | \\text{Marketing Channels}] = \\text{trend} + \\text{seasonality} + f_{\\text{social_media}}(x_{\\text{social_media}}) + f_{\\text{email}}(x_{\\text{email}}) + f_{\\text{tv}}(x_{\\text{tv}})\n\\]"
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#other-applications",
    "href": "presentations/bayesian_inference/index.html#other-applications",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Other applications",
    "text": "Other applications\n\nForecasting for Inventory Management: Estimating the probability of stock-outs and optimal reorder points.\nCensored Data Analysis: (e.g., survival analysis in medicine, reliability engineering)\nA/B Testing: Quantifying \\(P(\\text{Variant A &gt; Variant B})\\) and the magnitude of difference.\nHierarchical Models: Sharing information between groups (e.g., price elasticity across different products/regions, user behavior in different cohorts)."
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#conclusion",
    "href": "presentations/bayesian_inference/index.html#conclusion",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\nActs as regularization\nInference comes in many flavors: MCMC, Variational Inference, MAP.\nProbabilistic Programming Languages (PPLs) make it easy to implement complex models.\nProvide rich uncertainty quantification.\nNatural way to incorporate domain knowledge through priors."
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#thank-you",
    "href": "presentations/bayesian_inference/index.html#thank-you",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Thank you!",
    "text": "Thank you!\n\nJoin sktime and Prophetverse’s discord channel!"
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#motivation-why-bayesian",
    "href": "presentations/bayesian_inference/index.html#motivation-why-bayesian",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Motivation: Why Bayesian?",
    "text": "Motivation: Why Bayesian?\nWhich of these is a Bayesian statement, and which is Frequentist?\n\n\nA. There is a 95% probability that the true value \\(\\theta\\) lies in my interval \\([A, B]\\).\n\n\nB. There is 95% chance that my interval \\([A, B]\\) contains the true quantity \\(\\theta\\).\n\n\nIf \\([A, B]\\) is an interval generated by a model, and \\(\\theta\\) is the parameter of interest."
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#motivation-interpreting-intervals",
    "href": "presentations/bayesian_inference/index.html#motivation-interpreting-intervals",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Motivation: Interpreting Intervals",
    "text": "Motivation: Interpreting Intervals\nAnswer:\n\nA. (Bayesian): “There is a 95% probability that the true quantity \\(\\theta\\) lies in \\([A, B]\\)” * Treats \\(\\theta\\) as random, data as fixed. Probability statement about the parameter.\n\n\nB. (Frequentist): “There is 95% chance that \\([A, B]\\) contains the true quantity \\(\\theta\\)” * Treats \\(\\theta\\) as fixed, data (and thus interval) as random. Statement about the procedure: if repeated many times, 95% of such intervals would capture the true \\(\\theta\\).\n\n\nThe key difference: Bayesian credible intervals condition on the observed data. Frequentist confidence intervals consider the randomness of the data generation process."
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#visualizing-the-difference",
    "href": "presentations/bayesian_inference/index.html#visualizing-the-difference",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Visualizing the Difference",
    "text": "Visualizing the Difference\n\n\nAdapted from Jake VanderPlas. (Link to video)."
  },
  {
    "objectID": "presentations/bayesian_inference/index.html#motivation-3",
    "href": "presentations/bayesian_inference/index.html#motivation-3",
    "title": "Bayesian Inference - Introduction with Applications",
    "section": "Motivation",
    "text": "Motivation\nPrincipled Regularization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPriors act as a natural way to regularize models and prevent overfitting.\nFor example, Lasso and Ridge regressions are “bayesian”."
  }
]
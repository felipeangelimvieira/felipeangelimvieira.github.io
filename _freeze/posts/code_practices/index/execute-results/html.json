{
  "hash": "e99e39c6af014b6dbdf7e18f57a2832f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Can your code pass the test set?\"\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    toc-expand: 3\nauthor: \"Felipe Angelim\"\ndate: \"2025-05-27\"\ndraft: false\ncategories: [software design, machine learning]\nthumbnail: \"imgs/darkroom.png\"\njupyter: python3\n---\n\n\n\nCode is not meant to be written once and never touched again. It is meant to be read, understood, and modified over time. This is especially true in dynamic environments, where requirements change, new features are added, and bugs are fixed. In such environments, code design quality is not just a nice-to-have; it is essential for the long-term success of the project.\n\nHere, I wish to share some thoughts on code design qualities, focusing on what\nwe want, and not how to get there. This is not a guide on how to write good code, but rather a reflection on what good code design means in the context of software engineering and data science.\nThis can also be useful for managers, product owners, and stakeholders who do not code, but want to understand how quality impacts the product they are building.\n\nThe key takeaway I wish you to have is that **good code design is about resilience**. It is about not overfitting to our myopic view of the present, but rather about crafting a solution that can adapt and generalize to future requirements, just like a good machine learning model. It is about how efortless it is to adapt to changes.\n\n## The map is not the territory\n\n![](imgs/map_territory.png){width=30% fig-align=\"center\" style=\"border-radius: 40px;\"}\n\n\n**Product requirement is not the product you are building**. It is a simplification what\nyou want to build. Naturally, details can be missing and assumptions can be wrong, because sometimes\nreality is more complex than what we can express in words. Sometimes, we just don't know and that is okay. We just have to be cautious to not mistake the empty with the void.\n\n\n### Overfitting to training data\n\nOne might naively think that if a code does what it needs to do, it is good code. The thing is that current requirements might not reflect the long-term goals of a project. This though\ncan lead developers to write *easy* solutions and spaghetti code that work for the present moment but is blind to the bigger picture. \n\n\n![](imgs/illustration1.png){width=70% fig-align=\"center\" }\n\nThink about an abstract space of requirements. We see just a small fraction of\nthe long-term objectives. Focusing too much on what we know right now makes us overfit to the training data, failing to generalize to unseen examples. While this analogy to Machine Learning can sound too much, it is actually quite accurate: **the more complex a code is, the more it is prone to overfitting**. The more hand-tuned it is to current needs, the less it can adapt to future changes. \n\n::: {.callout-note}\n## Simple is not easy\n\nBut be careful, as [Rich Hickey](https://www.youtube.com/watch?v=rI8tNMsozo0) tells us, simple is not the same as easy. Simple is about not packing together unrelated concerns, it is about giving clear responsability to each component, it is about making it easy to modify and extend. Easy, on the other hand, is subjective, is what is closer to your knowledge and experience, it is what you are used to do, and not necessarily good. We should focus here on the code, and not on our perspective of it.\n:::\n\n\n![](imgs/illustration2.png){width=70% fig-align=\"center\"}\n\nThe easy will possibly lead to less effort on the short term, and we will easily deliver what needs to be delivered. Over time, however, the simples, generalizable code pays back, and do not lead to an infinite increase in effort to adapt to new requirements.\n\n::: {#04042a51 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){fig-align='center'}\n:::\n:::\n\n\n### The regret\n\nI wondered a lot about what would be the true objective function we are trying to optimize when writing code and creating solutions. As machine-learning models have a metrics to define their performance, code design quality must have a way to measure how well it is designed.\n\nI came to the conclusion that the regret is a good measure of code design quality. Regret is the cumulative extra effort when compared to the best possible solution, as there is no perfect solution. This notion is already used in some domains of Machine Learning, particularly in Multi-armed Bandits.\n\n::: {#6f964d50 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![Regret caused by overfitting](index_files/figure-html/cell-3-output-1.png){fig-align='center'}\n:::\n:::\n\n\n![](imgs/darkroom.png){width=50% fig-align=\"center\" style=\"border-radius: 40px;\"}\n\n\nThis is a common pitfall that reflects the lack of knowledge of existing unknowed unknowns.\nWe should practice [Epistemic humility](https://en.wikipedia.org/wiki/Epistemic_humility).\n\n---\n\n\nWhen talking or teaching about good code, people usually talk about principles and good practices.\nWhen focusing too much on \"how\", listeners and new programmers can mistaken it by \"what\" we\nare trying to build: **good software**.\n\nThen, you can ask: what *good* software is? I \nI have question in the back of my mind that I really struggle to answer: what good software design really is? People often talk about SOLID principles, but following these principles do not guarantee\ndesign quality, and can actually make harm when used without discipline. \n\n1.1  The “tiny functions” pathology\n\nUncle Bob/Clean Code vs. Cindy Sridharan’s Small Functions Considered Harmful³.\n\n## It is about the goals, not how to get there\n\nWriting good code is a lot like building a good model: it’s about resilience. It’s not just about making something that works today—it’s about crafting something that still works tomorrow, with minimal changes. In data science and software engineering alike, this is the essence of generalization.\n\nFor data scientists, the concept is familiar. Overfitting to current requirements can leave your code brittle and unable to cope with the next dataset, the next user story, or the next product feature. Models are expected to generalize. So should your code. Requirements drift, just as data does. The question is: does your design handle that drift, or does it collapse under it?\n\nBreaking code into components isn’t valuable on its own. Modularity for the sake of modularity is like adding noisy features to a model—it doesn’t help, and may even hurt. The point is not to split code, but to find components and contracts that protect your system from future change. Resilience is the goal.\n\nYou might ask: how can anyone know what future changes will come? Let’s break that into two cases:\n\t1.\tWhen future requirements are predictable.\nTake an e-commerce platform. Even with a minimal first version, you can safely assume features like shopping carts, order history, or payment gateways will eventually be added. That’s not guesswork—it’s experience. Your code should make room for those features from the outset.\n\t2.\tWhen future requirements are uncertain.\nHere, you work with a prediction horizon: an estimate of how far you can see into the future. Beyond that, it’s foggy. You won’t predict everything. But hopefully, what’s unpredictable is a smaller slice of the pie. Design for what’s stable. Encapsulate what might change. Use interfaces, boundaries, and contracts to contain the volatility.\n\nThis is where business knowledge comes in. The better you understand the problem domain, the sharper your predictions become. Code architecture is never built in isolation—it’s shaped by product owners, stakeholders, and managers. The quality of your design reflects how well you’ve translated their insights into structure.\n\nFrameworks and libraries exist because others have walked this path. Tools like scikit-learn, sktime, Langsmith, or ibis-framework represent distilled experience: abstractions that work across common problem spaces. When they match your needs, don’t reinvent—reuse, or at least learn from them.\n\nDesigning with Change in Mind\n\nThink of architecture as a search problem. You’re exploring a space of possible solutions. Try this exercise:\n\t1.\tDefine clearly the product you’re building and the problem you’re solving.\n\t2.\tAsk: what parts of this solution never change? What parts might?\n\t3.\tRepeat this question recursively for each component.\n\nTake a classic example: predicting customer churn using machine learning. Regardless of approach, the process always includes:\n\t•\tData ingestion: you need to bring data in.\n\t•\tPreprocessing: features must be engineered, targets aligned.\n\t•\tClassification: a model must be trained and used for prediction.\n\t•\tPostprocessing: results must be saved or served.\n\nZooming in on classification: models can change, but some operations won’t. Every solution must:\n\t•\tCreate a model.\n\t•\tFit it.\n\t•\tPredict outcomes.\n\nThese steps form a stable backbone. You can define contracts here:\n\t•\tcreate_model() → model\n\t•\tfit(model, X, y) → trained_model\n\t•\tpredict(trained_model, X) → predictions\n\nSounds familiar? It should—it’s essentially how scikit-learn is structured. These abstractions allow models to be swapped in and out with minimal disruption.\n\nNext, define the expected data structures at each step. This avoids cascading changes when upstream formats change. It’s a core strategy in future-proofing your code: reduce the surface area exposed to volatility.\n\nOne of the most unpredictable elements in data projects is input data structure. Tables, schemas, sources—they all vary across use cases. This shouldn’t ripple through your pipeline. Instead, introduce a uniformization layer: a stage where raw data is transformed into a consistent internal format. Think of it as an ETL boundary. Beyond this point, your code sees only familiar, stable shapes.\n\nThis is how you generalize—not just your models, but your design.\n\n\n\nFinal thought: ask yourself not just whether your code passes the current test suite, but whether it would pass the test set of future requirements. Can your architecture handle the real-world drift in needs, data, and expectations? If not, it might be time to refactor—not for elegance, but for endurance.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
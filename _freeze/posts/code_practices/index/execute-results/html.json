{
  "hash": "756c0c456823260ef3aa7deca9273e52",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Can your code pass the test set?\"\nauthor: \"Felipe Angelim\"\ndate: \"2025-05-27\"\ncategories: [software design, machine learning]\njupyter: python3\n---\n\n\nWhen talking or teaching about good code, people usually talk about principles and good practices.\nWhen focusing too much on \"how\", listeners and new programmers can mistaken it by \"what\" we\nare trying to build: **good software**.\n\nThen, you can ask: what *good* software is? I \nI have question in the back of my mind that I really struggle to answer: what good software design really is? People often talk about SOLID principles, but following these principles do not guarantee\ndesign quality, and can actually make harm when used without discipline. \n\n1.1  The “tiny functions” pathology\n\nUncle Bob/Clean Code vs. Cindy Sridharan’s Small Functions Considered Harmful³.\n\n## It is about the goals, not how to get there\n\nWriting good code is a lot like building a good model: it’s about resilience. It’s not just about making something that works today—it’s about crafting something that still works tomorrow, with minimal changes. In data science and software engineering alike, this is the essence of generalization.\n\nFor data scientists, the concept is familiar. Overfitting to current requirements can leave your code brittle and unable to cope with the next dataset, the next user story, or the next product feature. Models are expected to generalize. So should your code. Requirements drift, just as data does. The question is: does your design handle that drift, or does it collapse under it?\n\nBreaking code into components isn’t valuable on its own. Modularity for the sake of modularity is like adding noisy features to a model—it doesn’t help, and may even hurt. The point is not to split code, but to find components and contracts that protect your system from future change. Resilience is the goal.\n\nYou might ask: how can anyone know what future changes will come? Let’s break that into two cases:\n\t1.\tWhen future requirements are predictable.\nTake an e-commerce platform. Even with a minimal first version, you can safely assume features like shopping carts, order history, or payment gateways will eventually be added. That’s not guesswork—it’s experience. Your code should make room for those features from the outset.\n\t2.\tWhen future requirements are uncertain.\nHere, you work with a prediction horizon: an estimate of how far you can see into the future. Beyond that, it’s foggy. You won’t predict everything. But hopefully, what’s unpredictable is a smaller slice of the pie. Design for what’s stable. Encapsulate what might change. Use interfaces, boundaries, and contracts to contain the volatility.\n\nThis is where business knowledge comes in. The better you understand the problem domain, the sharper your predictions become. Code architecture is never built in isolation—it’s shaped by product owners, stakeholders, and managers. The quality of your design reflects how well you’ve translated their insights into structure.\n\nFrameworks and libraries exist because others have walked this path. Tools like scikit-learn, sktime, Langsmith, or ibis-framework represent distilled experience: abstractions that work across common problem spaces. When they match your needs, don’t reinvent—reuse, or at least learn from them.\n\nDesigning with Change in Mind\n\nThink of architecture as a search problem. You’re exploring a space of possible solutions. Try this exercise:\n\t1.\tDefine clearly the product you’re building and the problem you’re solving.\n\t2.\tAsk: what parts of this solution never change? What parts might?\n\t3.\tRepeat this question recursively for each component.\n\nTake a classic example: predicting customer churn using machine learning. Regardless of approach, the process always includes:\n\t•\tData ingestion: you need to bring data in.\n\t•\tPreprocessing: features must be engineered, targets aligned.\n\t•\tClassification: a model must be trained and used for prediction.\n\t•\tPostprocessing: results must be saved or served.\n\nZooming in on classification: models can change, but some operations won’t. Every solution must:\n\t•\tCreate a model.\n\t•\tFit it.\n\t•\tPredict outcomes.\n\nThese steps form a stable backbone. You can define contracts here:\n\t•\tcreate_model() → model\n\t•\tfit(model, X, y) → trained_model\n\t•\tpredict(trained_model, X) → predictions\n\nSounds familiar? It should—it’s essentially how scikit-learn is structured. These abstractions allow models to be swapped in and out with minimal disruption.\n\nNext, define the expected data structures at each step. This avoids cascading changes when upstream formats change. It’s a core strategy in future-proofing your code: reduce the surface area exposed to volatility.\n\nOne of the most unpredictable elements in data projects is input data structure. Tables, schemas, sources—they all vary across use cases. This shouldn’t ripple through your pipeline. Instead, introduce a uniformization layer: a stage where raw data is transformed into a consistent internal format. Think of it as an ETL boundary. Beyond this point, your code sees only familiar, stable shapes.\n\nThis is how you generalize—not just your models, but your design.\n\n\n\nFinal thought: ask yourself not just whether your code passes the current test suite, but whether it would pass the test set of future requirements. Can your architecture handle the real-world drift in needs, data, and expectations? If not, it might be time to refactor—not for elegance, but for endurance.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
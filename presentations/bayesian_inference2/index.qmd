---
title: "Bayesian Inference - Introduction with Applications"
author: "Felipe Angelim"
format: 
  revealjs:
    theme: [default, custom.scss] # Example: add a custom.scss for styling
    slide-number: true
    #chalkboard: true
    #logo: logo.png # Optional: if you have a logo
    embed-resources: true # Good for sharing a single HTML file
    incremental: true # Makes lists appear one item at a time by default
    # toc: true # Adds a table of contents
    # toc-depth: 2
    width: 1280
    height: 720
---

## About me

:::: {.columns style="align-items: center; justify-content: center; width: 100%;"}
  
::: {.column style="width: 20%; display: flex; align-items: center; justify-content: center;"}
![](profile.png){width="300px" fig-align="center"}
:::

::: {.column style="display: flex; flex-direction: column; align-items: center; gap: 0.3em;"}
<p class="no-margin" style="font-size: 1.8em;">Felipe Angelim</p>
<p class="no-margin" style="font-size: 0.8em;">Tech Lead @ Mercado Libre</p>
<p class="no-margin" style="font-size: 0.8em;">Core Dev @ Sktime</p>
<p class="no-margin" style="font-size: 0.8em;">Creator/Dev @ Prophetverse</p>
<p class="no-margin" style="font-size: 0.8em;">
  <a href="https://felipeangelim.com">felipeangelim.com</a>
</p>
:::

::::
---

## Agenda

1.  Motivation: Why Bayesian?
2.  Bayesian Inference theory
3.  Bayesian Inference applications

---

## Motivation

![](users-data.png)


## Motivation: Why Bayesian?

Let's start with a question...

. . .

**Which of these is a Bayesian statement, and which is Frequentist**?

::: {.styled-card}
**A.** There is a 95% probability that the **true value $\theta$ lies in my interval $[A, B]$**.
:::

::: {.styled-card}
**B.** There is 95% chance that **my interval $[A, B]$ contains the true quantity $\theta$**.
:::

::: {style="font-size: 0.6em;"}
*If $[A, B]$ is an interval generated by a model, and $\theta$ is the parameter of interest.*
:::

---

## Motivation: Interpreting Intervals

::: {.nonincremental}
**Answer:**
:::

> **A. (Bayesian): "There is a 95% probability that the true quantity $\theta$ lies in $[A, B]$"**
>    * Treats $\theta$ as random, data as fixed. Probability statement about the parameter.

> **B. (Frequentist): "There is 95% chance that $[A, B]$ contains the true quantity $\theta$"**
>    * Treats $\theta$ as fixed, data (and thus interval) as random. Statement about the procedure: if repeated many times, 95% of such intervals would capture the true $\theta$.

::: footer
The key difference: Bayesian credible intervals condition on the observed data. Frequentist confidence intervals consider the randomness of the data generation process.
:::

---

## Visualizing the Difference

![](intervals.png){fig-align="center" width="600"}


::: {.caption}
Adapted from Jake VanderPlas. ([Link to video](http://www.youtube.com/watch?v=KhAUfqhLakw)).
:::

---

## Motivation

### Probabilistic Programming is Powerful


:::: {.columns}

::: {.column}

![](tf-logo.png)

![](pymc-logo.png)


:::

::: {.column}

![](numpyro-logo.png)

:::


::: {.column}

* Easy implementation of structural models, e.g., epidemiology, climate,
  economics, etc.
* Fast inference on large datasets.

:::

::::


---

## Motivation

### Principled Regularization


:::: {.columns}

::: {.column width="70%" style="width: 70%; display: flex; flex-direction: row; align-items: center;"}

![](regularization1.png){fig-align="center" width="100%"}

![](regularization2.png){fig-align="center" width="100%"}
:::

::: {.column width="30%" style="width: 30%;"}

* Priors act as a natural way to regularize models and prevent overfitting.
* For example, Lasso and Ridge regressions are "bayesian".
:::

::::

---

## Motivation

### Rich Uncertainty Quantification

:::: {.columns}


::: {.column width="70%"}
![](custom-trend-plot.png){fig-align="center" width="100%"}
:::

::: {.column width="30%"}

* Obtain full posterior distributions for parameters.
* Calculate credible intervals, probabilities of hypotheses and predict distributions.
:::

::::


---

## Motivation: Concrete Use-Cases (Overview)

Bayesian methods shine in areas like:

* **Censored Data Analysis:** (e.g., survival analysis in medicine, reliability engineering)
* **A/B Testing:** Quantifying $P(\text{Variant A > Variant B})$ and the magnitude of difference.
* **Hierarchical Models:** Sharing information between groups (e.g., price elasticity across different products/regions, user behavior in different cohorts).
* **Time Series Forecasting:** Incorporating priors for structure, quantifying uncertainty in future predictions (e.g., probability of stock-outs).

---

# The theory

---

## Bayes' Rule: The Core Idea

The foundation of Bayesian inference:

$$
\underbrace{P(\theta|X)}_{\text{Posterior}} = \frac{\overbrace{P(X|\theta)}^{\text{Likelihood}} \overbrace{P(\theta)}^{\text{Prior}}}{\underbrace{P(X)}_{\text{Evidence}}}
$$

Relates the probability of parameters $\theta$ given data $X$ to our prior beliefs and the likelihood of the data.

---

## Bayes' Rule: The Proportional Form

For parameter estimation, we often use the proportional form:

$$
\underbrace{P(\theta|X)}_{\text{Posterior}} \propto \underbrace{P(X|\theta)}_{\text{Likelihood}} \underbrace{P(\theta)}_{\text{Prior}}
$$

* The **Evidence** $P(X)$ is a normalization constant. It doesn't depend on $\theta$, so it's often ignored for finding the *shape* and *relative probabilities* of the posterior.

![](plp.png){fig-align="center" width="100%"}

---

## Bayes' Rule: Illustrative Example - Box with Coloured Balls {.smaller}

Imagine a box with an unknown proportion $\theta$ of red balls. We draw $N$ balls and observe $K$ red ones.

:::: {.columns}

::: {.column}

![](box.png){fig-align="center" width="300"}

$$
\theta \sim \text{Uniform}(0, 1) \quad \text{(Prior)}
$$

$$
X | \theta \sim \text{Binomial}(N, \theta) \quad \text{(Likelihood)}
$$

:::

::: {.column}

<p style="font-size: 0.8em; margin-top: 0.5em; margin-bottom: 2em;">Numpyro model<br></p>
```python
import numpyro
import numpyro.distributions as dist

def bernoulli_model(data=None):
    """Uniform-prior Bernoulli model p ~ U(0,1)."""
    theta = numpyro.sample("theta", dist.Uniform(0.0, 1.0))
    with numpyro.plate("obs", data.shape[0]):
        numpyro.sample("y", dist.Bernoulli(probs=theta), obs=data)
```

:::

::::

::: footer
This is a "conjugate prior" example where the math is clean. Most real-world models aren't this simple!
:::

---

## Bernoulli implementation in Numpyro



